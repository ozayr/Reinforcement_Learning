{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations,permutations\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "class Tictactoe:\n",
    "#     can play the game using non GUI , just run the play() method\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.states = self.get_states()\n",
    "        self.wins = self.get_wins()\n",
    "        self.actions = dict(enumerate(map(chr,range(ord('a'),ord('i')+1)))) # this felt good \n",
    "        self.Q = np.zeros((len(self.states),9) , np.int)\n",
    "        self.training_done = mp.Queue()\n",
    "        \n",
    "    @staticmethod    \n",
    "    def get_states():\n",
    "    #     there has to be atleast 1 0 ie 1 empty space in the states as all states filled is end game which has no next state,\n",
    "    #     the states have to encode the agents next move so where, either there is a single 1 meaning\n",
    "    #     that the agents turn is next or the number of agent plays (2's) cannot be even as that means it is the other players turn\n",
    "\n",
    "        comb = list(set(combinations(np.repeat( [0,1,2] , 9),9)))\n",
    "        valid_configs = []\n",
    "        for com in comb:\n",
    "            vals, counts = np.unique(com,return_counts=True)\n",
    "    #         print(vals, counts)\n",
    "            if (0 in vals):\n",
    "                if len(vals) == 2:\n",
    "                    if counts[1] == 1 and vals[1] != 2:\n",
    "                        valid_configs.append(com)\n",
    "\n",
    "                elif len(vals) > 2:\n",
    "                    if  (counts[2]==counts[1]-1) or (counts[1]==counts[2]) : \n",
    "                         valid_configs.append(com)\n",
    "                else:\n",
    "                     valid_configs.append(com)\n",
    "\n",
    "\n",
    "        #         print(com,vals , counts)\n",
    "        valid_states = []\n",
    "        for config in valid_configs:\n",
    "            valid_states.append(list(set(permutations(config))))\n",
    "\n",
    "        flat_list = [item for sublist in valid_states for item in sublist]\n",
    "        state_dict = dict()\n",
    "        for i,state in enumerate(sorted(flat_list)):\n",
    "            state_dict[i] = state\n",
    "        return state_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def check(test,array):\n",
    "        return [True for win in array if set(win).issubset(test)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_wins():\n",
    "        grid  = np.array(range(9)).reshape(3,3)\n",
    "        wins = []\n",
    "        wins  = [grid[i] for i in range(3)]\n",
    "        wins  +=  [grid[:,i] for i in range(3)]\n",
    "        wins  += [grid.diagonal()]\n",
    "        wins  += [np.fliplr(grid).diagonal()]\n",
    "        return wins\n",
    "    \n",
    "    def run_episode(self,episode_length,gamma,alpha ,epsilon ,win_reward, lose_reward, draw_reward,play_reward,queue_for_q):\n",
    "    \n",
    "        Q = np.zeros((len(self.states),9) , np.int)\n",
    "        states = self.states\n",
    "        \n",
    "        for i in range(episode_length):\n",
    "    #         simulate random states\n",
    "            current_state_selector = np.random.randint(0,i%len(states)+1)  \n",
    "            current_state = np.array(states[current_state_selector])\n",
    "    #         get playable blocks\n",
    "            available_blocks = np.where(np.array(current_state) == 0)[0]\n",
    "    #         epsilon greedy methond ,explore , exploit\n",
    "            if  np.random.random() > epsilon:   \n",
    "\n",
    "                avialable_q_vals =   np.array([Q[current_state_selector][available_blocks],available_blocks])\n",
    "#                 print(avialable_q_vals[0],\"q\")\n",
    "#                 print(avialable_q_vals[1],\"a\")\n",
    "                if len(np.unique(avialable_q_vals[0])) == 1:\n",
    "                    action_to_take_agent = np.random.choice(available_blocks)\n",
    "\n",
    "                else:\n",
    "                 \n",
    "                    action_to_take_agent = avialable_q_vals[1,np.argmax(avialable_q_vals[0])]\n",
    "\n",
    "            else:\n",
    "\n",
    "                action_to_take_agent = np.random.choice(available_blocks) # index/pos at which to play\n",
    "\n",
    "            action_taken_state = current_state.copy()\n",
    "            action_taken_state[action_to_take_agent] = 2\n",
    "\n",
    "        #     did i win ?\n",
    "            plays = np.where(np.array(action_taken_state) == 2)[0]\n",
    "            reward = play_reward\n",
    "            terminal_state = False\n",
    "\n",
    "            if self.check(plays,self.wins):# win\n",
    "                reward = win_reward\n",
    "                terminal_state = True\n",
    "\n",
    "            elif 0 not in action_taken_state:# draw\n",
    "                reward = draw_reward\n",
    "                terminal_state = True\n",
    "            else:\n",
    "    #         rival play\n",
    "\n",
    "                posible_rival_actions  = np.where(np.array(action_taken_state) == 0)[0]# get possible plays\n",
    "                action_to_take_rival = np.random.choice(posible_rival_actions) # index/pos at which to play\n",
    "                action_taken_state[action_to_take_rival] = 1\n",
    "\n",
    "                plays = np.where(np.array(action_taken_state) == 1)[0]\n",
    "                if self.check(plays,self.wins):\n",
    "                    reward = lose_reward\n",
    "                    terminal_state = True\n",
    "                elif (0 not in action_taken_state):\n",
    "                    reward = draw_reward\n",
    "                    terminal_state = True\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            if terminal_state:\n",
    "                TD = reward\n",
    "            else:    \n",
    "                next_state_locator = list(states.values()).index(tuple(action_taken_state))\n",
    "                TD  = reward + gamma *  Q[next_state_locator][np.argmax(Q[next_state_locator])] - Q[current_state_selector][action_to_take_agent]\n",
    "\n",
    "            Q[current_state_selector][action_to_take_agent] +=  alpha * TD\n",
    "\n",
    "        else:\n",
    "            queue_for_q.put(Q)\n",
    "\n",
    "\n",
    "    def train(self,episode_length = 5000,nprocs = None,gamma= 0.6,alpha = 0.99,epsilon = 0.4 , win_reward = 100 , lose_reward = -100 , draw_reward = -50 , play_reward = -1):        \n",
    "    \n",
    "        sessions = []\n",
    "        queue_for_q = mp.Queue()\n",
    "        if not nprocs:\n",
    "            nprocs = mp.cpu_count()\n",
    "        \n",
    "        \n",
    "#         create multiple processes to run the training episodes in paralelle\n",
    "\n",
    "        for i in range(nprocs):\n",
    "            p = mp.Process(target=self.run_episode,args=(episode_length,gamma,alpha,epsilon ,win_reward, lose_reward, draw_reward,play_reward,queue_for_q))\n",
    "            p.start()\n",
    "            sessions.append(p)\n",
    "\n",
    "#         get the Q values from the Queue lol\n",
    "        for i in range(nprocs):\n",
    "            self.Q += queue_for_q.get()\n",
    "\n",
    "#         wait for finish\n",
    "        for session in sessions:\n",
    "            session.join()\n",
    "        \n",
    "#         average over all that we have learned , maybe not best method ? \n",
    "        self.Q = self.Q//nprocs\n",
    "        self.training_done.put(1)\n",
    "       \n",
    "    \n",
    "    @staticmethod\n",
    "    def show_game_state(state):\n",
    "        \n",
    "        available_blocks = np.where(np.array(state) == 0 )[0]\n",
    "        state = np.array(state).reshape(3,3)\n",
    "\n",
    "        print('a|b|c',state[0])\n",
    "        print('d|e|f',state[1])\n",
    "        print('g|h|i',state[2])\n",
    "        return available_blocks\n",
    "    \n",
    "    def play(self):# manual play\n",
    "        game_state = np.zeros(9,int)\n",
    "        blocks = {j:i for i,j in self.actions.items()}   \n",
    "        while 1:\n",
    "#             GAME STATE\n",
    "#             ==================================================================\n",
    "            availble_blocks = self.show_game_state(game_state)\n",
    "            availble_blocks = [self.actions[i] for i in availble_blocks]\n",
    "            print('choose block to play in:',availble_blocks)\n",
    "#             ==================================================================\n",
    "#             HUMAN PLAY\n",
    "            human_action = input('enter block to play in >')\n",
    "\n",
    "            if human_action not in availble_blocks:\n",
    "                print('invalid')\n",
    "                continue\n",
    "            else:\n",
    "                human_action = blocks[human_action] # convert letter to index\n",
    "                game_state[human_action] = 1\n",
    "\n",
    "                plays = np.where(np.array(game_state) == 1)[0]\n",
    "\n",
    "                if self.check(plays,self.wins):\n",
    "                    print('HUMAN WINS')\n",
    "                    self.show_game_state(game_state)\n",
    "                    break\n",
    "                elif 0 not in game_state:\n",
    "                    print('DRAW')\n",
    "                    break\n",
    "                    \n",
    "#                 ================================================================================\n",
    "#                 AGENT PLAY\n",
    "                what_state = list(self.states.values()).index(tuple(game_state))\n",
    "                available_blocks = np.where(np.array(game_state) == 0)[0]\n",
    "                avialable_q_vals =   np.array([self.Q[what_state][available_blocks],available_blocks])\n",
    "\n",
    "                if len(np.unique(avialable_q_vals[0])) == 1:\n",
    "                    agent_action = np.random.choice(available_blocks)\n",
    "                else:\n",
    "                    agent_action = avialable_q_vals[1,np.argmax(avialable_q_vals[0])]\n",
    "\n",
    "                game_state[agent_action] = 2\n",
    "\n",
    "                plays = np.where(np.array(game_state) == 2)[0]\n",
    "                if self.check(plays,self.wins):\n",
    "                    print('AGENT WINS')\n",
    "                    self.show_game_state(game_state)\n",
    "                    break\n",
    "                elif 0 not in game_state:\n",
    "                    print('DRAW')\n",
    "                    break\n",
    "    \n",
    "        \n",
    "        \n",
    "    def agent_play(self,game_state):\n",
    "\n",
    "        what_state = list(self.states.values()).index(tuple(game_state))\n",
    "        available_blocks = np.where(np.array(game_state) == 0)[0]\n",
    "        available_q_vals =   np.array([self.Q[what_state][available_blocks],available_blocks])\n",
    "\n",
    "        if len(np.unique(available_q_vals[0])) == 1:\n",
    "            agent_action = np.random.choice(available_blocks)\n",
    "        else:\n",
    "            agent_action = avialable_q_vals[1,np.argmax(available_q_vals[0])]\n",
    "\n",
    "        return self.actions[agent_action]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avialable_q_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c75f4630bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0magent_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbutton_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_play\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_play\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1dc02bdd07d2>\u001b[0m in \u001b[0;36magent_play\u001b[0;34m(self, game_state)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mavailable_q_vals\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhat_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mavailable_blocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavailable_blocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavialable_q_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0magent_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avialable_q_vals' is not defined"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "import threading\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "sg.theme('Black')\n",
    "agent = Tictactoe()\n",
    "\n",
    "layout = [\n",
    "    [sg.Text('',size = (20,1) , key = 'end_state')],\n",
    "    [sg.Frame(layout=[\n",
    "    [sg.Button(\"\",size = (5,1),enable_events = True,key = 'a'),sg.Button(\"\",size = (5,1),enable_events = True,key = 'b'),sg.Button(\"\",size = (5,1),enable_events = True,key = 'c')],\n",
    "    [sg.Button(\"\",size = (5,1),enable_events = True,key = 'd'),sg.Button(\"\",size = (5,1),enable_events = True,key = 'e'),sg.Button(\"\",size = (5,1),enable_events = True,key = 'f')],\n",
    "    [sg.Button(\"\",size = (5,1),enable_events = True,key = 'g'),sg.Button(\"\",size = (5,1),enable_events = True,key = 'h'),sg.Button(\"\",size = (5,1),enable_events = True,key = 'i')],\n",
    "    ],title = \"\")],\n",
    "    [sg.Frame(layout=[\n",
    "    [sg.Text('Training Intensity:')],\n",
    "    [sg.Slider(default_value = 1000,range=(0, 10000000), size=(21,10), orientation='h', key='training_intensity')],\n",
    "    \n",
    "    [sg.Text('Alpha:',size = (len('Epsilon:'),1)),sg.Input(size = (5,1),key = 'alpha',default_text='0.99'),sg.Text('Gamma:'),sg.Input(size = (5,1),key = 'gamma',default_text='0.6')],\n",
    "    [sg.Text('Epsilon:'),sg.Input(size = (5,1),key = 'epsilon',default_text='0.4')] ,\n",
    "#     win_reward = 100 , lose_reward = -100 , draw_reward = -50 , play_reward = -1\n",
    "    [sg.Text('win:',size = (len('draw:'),1)),sg.Input(size = (5,1),key = 'win',default_text='100'),sg.Text('lose:'),sg.Input(size = (5,1),key = 'lose',default_text='-100')],\n",
    "    [sg.Text('draw:',size = (len('draw:'),1)),sg.Input(size = (5,1),key = 'draw',default_text='-50'),sg.Text('play:'),sg.Input(size = (5,1),key = 'play',default_text='-1')],\n",
    "    \n",
    "    [sg.Text('',size= (20,1),key='training_status')],\n",
    "    ],title = 'Parameters') ],\n",
    "    [sg.Button('Train me'),sg.Button('exit'),sg.Button('reset')],\n",
    "    \n",
    "      \n",
    "]\n",
    "\n",
    "window = sg.Window('TicTacToe',layout)\n",
    "#              player empty agent\n",
    "player_map = {1:'HUMAN' , 2:'AGENT SMITH'}\n",
    "symbol_map = {'o':1 , '':0 , 'x':2  }\n",
    "\n",
    "isTraining = False\n",
    "\n",
    "def evaluate(who,button_states):\n",
    "    \n",
    "    plays = np.where(np.array(button_states) == who)[0]\n",
    "    if agent.check(plays,agent.wins):\n",
    "        window['end_state'].update(f'{player_map[who]} WINS')\n",
    "        return 0\n",
    "    elif 0 not in button_states:\n",
    "        window['end_state'].update('DRAW')\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "while 1:\n",
    "    \n",
    "    event, value = window.read(timeout = 100)\n",
    "    \n",
    "    if event == 'exit':\n",
    "        window.close()\n",
    "        break\n",
    "    elif event == 'reset':\n",
    "        [window[chr(x)].update('',disabled = False) for x in range(ord('a'),ord('i')+1)]\n",
    "        [window[chr(x)].update(button_color = ('black','white'))  for x in range(ord('a'),ord('i')+1)]\n",
    "        window['end_state'].update('')   \n",
    "      \n",
    "    elif event == 'Train me':\n",
    "        steps = int(value['training_intensity'])\n",
    "        alpha = float(value['alpha'])\n",
    "        gamma = float(value['gamma'])\n",
    "        epsilon = float(value['epsilon'])\n",
    "                      \n",
    "        t1 = threading.Thread(target= agent.train , args = (steps,None,gamma,alpha,epsilon) , daemon = True)\n",
    "        t1.start()\n",
    "        window['training_status'].update('...Training....')\n",
    "#         agent.train(100000)\n",
    "        isTraining = True\n",
    "        window['Train me'].update(disabled = True)\n",
    "    elif event.isalpha() :\n",
    "        window[event].update('o')\n",
    "        window[event].update(disabled = True)\n",
    "        window[event].update(button_color = ('black','red') )\n",
    "        button_states = [symbol_map[window[chr(x)].GetText()]  for x in range(ord('a'),ord('i')+1)]\n",
    "        if not evaluate(1,button_states):\n",
    "            [window[chr(x)].update(button_color = ('black','white'),disabled=True)  for x in range(ord('a'),ord('i')+1)]\n",
    "            continue\n",
    "        \n",
    "        agent_play = agent.agent_play(button_states)\n",
    "        window[agent_play].update('x')\n",
    "        window[agent_play].update(disabled = True)\n",
    "        window[agent_play].update(button_color = ('black','blue'))\n",
    "        button_states = [symbol_map[window[chr(x)].GetText()]  for x in range(ord('a'),ord('i')+1)]\n",
    "        if not evaluate(2,button_states):\n",
    "            [window[chr(x)].update(button_color = ('black','white'),disabled=True)  for x in range(ord('a'),ord('i')+1)]\n",
    "            continue\n",
    "    \n",
    "    if isTraining:\n",
    "        \n",
    "        try:\n",
    "            agent.training_done.get_nowait()\n",
    "            window['training_status'].update('...DONE....')\n",
    "            isTraining = False\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_ticTacToe import run_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
